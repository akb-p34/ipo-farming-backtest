{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ IPO Farming Backtest System - Enhanced Edition\n",
    "\n",
    "**Comprehensive analysis of ALL IPOs since 2015 with advanced features**\n",
    "\n",
    "## ‚ú® New Features\n",
    "- **Full historical analysis** - All IPOs since Jan 1, 2015 (including delisted)\n",
    "- **Checkpoint/Resume** - Handles IBKR rate limits gracefully\n",
    "- **Organized outputs** - Each backtest in timestamped folder with metadata\n",
    "- **Automatic abstracts** - High-level summaries of findings\n",
    "- **Master tracking** - Compare results across multiple backtests\n",
    "\n",
    "## üìä What This Does\n",
    "1. Loads all IPOs since 2015 from Jay Ritter database\n",
    "2. Fetches intraday data (IBKR or simulated)\n",
    "3. Tests 91+ trading windows\n",
    "4. Generates comprehensive reports with metadata\n",
    "5. Tracks results across multiple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required packages\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "packages = {\n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'openpyxl': 'openpyxl',\n",
    "    'pytz': 'pytz',\n",
    "    'tqdm': 'tqdm',\n",
    "    'ib_insync': 'ib-insync',\n",
    "    'yfinance': 'yfinance'\n",
    "}\n",
    "\n",
    "for import_name, install_name in packages.items():\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"‚úì {import_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {install_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", install_name])\n",
    "        print(f\"‚úì {install_name} installed\")\n",
    "\n",
    "# Now import everything\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration & Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'DATA_MODE': 'HYBRID',  # 'SIMULATION', 'IBKR', or 'HYBRID' (IBKR with fallback)\n",
    "    'START_DATE': '2015-01-01',  # Analyze IPOs since this date\n",
    "    'END_DATE': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'INCLUDE_DELISTED': True,  # Include delisted companies\n",
    "    \n",
    "    # Processing settings\n",
    "    'MAX_TICKERS': None,  # None for all, or specify number\n",
    "    'BATCH_SIZE': 10,  # Process in batches to manage memory\n",
    "    'CHECKPOINT_ENABLED': True,  # Save progress for resume capability\n",
    "    \n",
    "    # IBKR Settings\n",
    "    'IBKR': {\n",
    "        'HOST': '127.0.0.1',\n",
    "        'PORT': 7497,  # 7497 for Paper, 7496 for Live\n",
    "        'CLIENT_ID': random.randint(1, 999),\n",
    "        'REQUEST_DELAY': 5.0,  # Base delay between requests\n",
    "        'PACING_DELAY': 15.0,  # Additional delay after errors\n",
    "        'MAX_RETRIES': 3,\n",
    "        'BACKOFF_FACTOR': 2.0  # Exponential backoff multiplier\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    'OUTPUT_BASE': 'outputs',\n",
    "    'TEST_PREFIX': 'backtest',\n",
    "    'GENERATE_ABSTRACT': True,\n",
    "    'GENERATE_VISUALIZATIONS': True,\n",
    "    'TRACK_MASTER_LOG': True\n",
    "}\n",
    "\n",
    "# Create test directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "test_dir = Path(CONFIG['OUTPUT_BASE']) / f\"{CONFIG['TEST_PREFIX']}_{timestamp}\"\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(test_dir / 'config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìä Test Configuration:\")\n",
    "print(f\"   ‚Ä¢ Date Range: {CONFIG['START_DATE']} to {CONFIG['END_DATE']}\")\n",
    "print(f\"   ‚Ä¢ Data Mode: {CONFIG['DATA_MODE']}\")\n",
    "print(f\"   ‚Ä¢ Include Delisted: {CONFIG['INCLUDE_DELISTED']}\")\n",
    "print(f\"   ‚Ä¢ Test Directory: {test_dir}\")\n",
    "print(f\"\\n‚úÖ Configuration saved to {test_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load IPO Universe (2015-Present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ipo_universe_full():\n",
    "    \"\"\"Load all IPOs since 2015 from Jay Ritter database\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load from Excel\n",
    "        df = pd.read_excel('IPO-age.xlsx', sheet_name='1975-2024', dtype={'offer date': str})\n",
    "        \n",
    "        # Parse dates\n",
    "        df['IPO_Date'] = pd.to_datetime(df['offer date'], format='%Y%m%d', errors='coerce')\n",
    "        \n",
    "        # Filter by date range\n",
    "        df = df[(df['IPO_Date'] >= CONFIG['START_DATE']) & \n",
    "                (df['IPO_Date'] <= CONFIG['END_DATE'])]\n",
    "        \n",
    "        # Clean ticker symbols\n",
    "        df = df.dropna(subset=['Ticker'])\n",
    "        df['Ticker'] = df['Ticker'].astype(str).str.strip().str.upper()\n",
    "        \n",
    "        # Rename columns\n",
    "        df = df.rename(columns={\n",
    "            'IPO name': 'Company',\n",
    "            'Offer Price': 'IPO_Price'\n",
    "        })\n",
    "        \n",
    "        # Add status (check if currently trading)\n",
    "        print(\"\\nüîç Checking ticker status (active/delisted)...\")\n",
    "        df['Status'] = 'Unknown'\n",
    "        \n",
    "        # Quick check using yfinance (in batches to avoid rate limits)\n",
    "        for i in tqdm(range(0, len(df), 50), desc=\"Checking status\"):\n",
    "            batch = df.iloc[i:i+50]\n",
    "            for _, row in batch.iterrows():\n",
    "                try:\n",
    "                    ticker = yf.Ticker(row['Ticker'])\n",
    "                    info = ticker.info\n",
    "                    if info and 'symbol' in info:\n",
    "                        df.loc[df['Ticker'] == row['Ticker'], 'Status'] = 'Active'\n",
    "                    else:\n",
    "                        df.loc[df['Ticker'] == row['Ticker'], 'Status'] = 'Delisted'\n",
    "                except:\n",
    "                    df.loc[df['Ticker'] == row['Ticker'], 'Status'] = 'Delisted'\n",
    "            time.sleep(1)  # Small delay between batches\n",
    "        \n",
    "        # Select columns\n",
    "        universe = df[['Ticker', 'Company', 'IPO_Date', 'IPO_Price', 'Status']].copy()\n",
    "        \n",
    "        # Handle missing IPO prices\n",
    "        universe['IPO_Price'] = pd.to_numeric(universe['IPO_Price'], errors='coerce')\n",
    "        universe['IPO_Price'].fillna(universe['IPO_Price'].median(), inplace=True)\n",
    "        \n",
    "        # Apply max ticker limit if specified\n",
    "        if CONFIG['MAX_TICKERS']:\n",
    "            universe = universe.head(CONFIG['MAX_TICKERS'])\n",
    "        \n",
    "        return universe\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading IPO data: {e}\")\n",
    "        print(\"Using sample data instead...\")\n",
    "        \n",
    "        # Fallback sample data\n",
    "        sample_data = [\n",
    "            ('UBER', 'Uber', '2019-05-10', 45.00, 'Active'),\n",
    "            ('LYFT', 'Lyft', '2019-03-29', 72.00, 'Active'),\n",
    "            ('ABNB', 'Airbnb', '2020-12-10', 68.00, 'Active'),\n",
    "            ('DASH', 'DoorDash', '2020-12-09', 102.00, 'Active'),\n",
    "            ('SNOW', 'Snowflake', '2020-09-16', 120.00, 'Active'),\n",
    "            ('COIN', 'Coinbase', '2021-04-14', 250.00, 'Active'),\n",
    "            ('HOOD', 'Robinhood', '2021-07-29', 38.00, 'Active'),\n",
    "            ('RIVN', 'Rivian', '2021-11-10', 78.00, 'Active'),\n",
    "        ]\n",
    "        \n",
    "        universe = pd.DataFrame(sample_data, \n",
    "                               columns=['Ticker', 'Company', 'IPO_Date', 'IPO_Price', 'Status'])\n",
    "        universe['IPO_Date'] = pd.to_datetime(universe['IPO_Date'])\n",
    "        return universe\n",
    "\n",
    "# Load IPO universe\n",
    "print(\"üìö Loading IPO universe...\")\n",
    "ipo_universe = load_ipo_universe_full()\n",
    "\n",
    "# Save universe to test directory\n",
    "ipo_universe.to_csv(test_dir / 'ipo_universe.csv', index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä IPO Universe Summary:\")\n",
    "print(f\"   ‚Ä¢ Total IPOs: {len(ipo_universe)}\")\n",
    "print(f\"   ‚Ä¢ Active: {(ipo_universe['Status'] == 'Active').sum()}\")\n",
    "print(f\"   ‚Ä¢ Delisted: {(ipo_universe['Status'] == 'Delisted').sum()}\")\n",
    "print(f\"   ‚Ä¢ Unknown: {(ipo_universe['Status'] == 'Unknown').sum()}\")\n",
    "print(f\"   ‚Ä¢ Date Range: {ipo_universe['IPO_Date'].min().date()} to {ipo_universe['IPO_Date'].max().date()}\")\n",
    "print(f\"\\nüìä By Year:\")\n",
    "ipo_universe['Year'] = ipo_universe['IPO_Date'].dt.year\n",
    "year_counts = ipo_universe.groupby('Year').size()\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"   {year}: {count} IPOs\")\n",
    "\n",
    "print(f\"\\nüíæ Universe saved to {test_dir}/ipo_universe.csv\")\n",
    "ipo_universe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Collection with Checkpoint System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPODataCollector:\n",
    "    \"\"\"Handles data collection with checkpointing and rate limit management\"\"\"\n",
    "    \n",
    "    def __init__(self, test_dir, config):\n",
    "        self.test_dir = Path(test_dir)\n",
    "        self.config = config\n",
    "        self.checkpoint_file = self.test_dir / 'checkpoint.json'\n",
    "        self.data_dir = self.test_dir / 'ticker_data'\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Load checkpoint if exists\n",
    "        self.checkpoint = self.load_checkpoint()\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load existing checkpoint or create new\"\"\"\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\n",
    "            'processed': [],\n",
    "            'failed': [],\n",
    "            'last_processed': None,\n",
    "            'total_requests': 0,\n",
    "            'start_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save current checkpoint\"\"\"\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(self.checkpoint, f, indent=2)\n",
    "    \n",
    "    def generate_ipo_day_data(self, ticker, ipo_date, ipo_price):\n",
    "        \"\"\"Generate realistic simulated IPO day data\"\"\"\n",
    "        np.random.seed(hash(ticker) % 2**32)\n",
    "        \n",
    "        # IPO day patterns\n",
    "        pop = np.random.uniform(0.9, 1.5)  # Opening pop\n",
    "        volatility = np.random.uniform(0.003, 0.008)\n",
    "        trend = np.random.uniform(-0.0002, 0.0005)\n",
    "        \n",
    "        # Generate time series\n",
    "        eastern = pytz.timezone('America/New_York')\n",
    "        start_time = pd.Timestamp(ipo_date).replace(hour=9, minute=30)\n",
    "        end_time = pd.Timestamp(ipo_date).replace(hour=16, minute=0)\n",
    "        times = pd.date_range(start_time, end_time, freq='1min')\n",
    "        \n",
    "        # Generate prices\n",
    "        open_price = ipo_price * pop\n",
    "        prices = [open_price]\n",
    "        \n",
    "        for i in range(1, len(times)):\n",
    "            hour = times[i].hour\n",
    "            \n",
    "            # Intraday patterns\n",
    "            if hour < 10:\n",
    "                vol_mult = 1.5\n",
    "                trend_mult = -0.5\n",
    "            elif hour < 12:\n",
    "                vol_mult = 1.2\n",
    "                trend_mult = 0.5\n",
    "            elif hour < 14:\n",
    "                vol_mult = 0.8\n",
    "                trend_mult = 1.0\n",
    "            else:\n",
    "                vol_mult = 1.1\n",
    "                trend_mult = 1.5\n",
    "            \n",
    "            change = np.random.normal(trend * trend_mult, volatility * vol_mult)\n",
    "            new_price = prices[-1] * (1 + change)\n",
    "            prices.append(max(new_price, ipo_price * 0.5))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'datetime': times,\n",
    "            'open': prices,\n",
    "            'high': np.array(prices) * (1 + np.abs(np.random.normal(0, 0.002, len(prices)))),\n",
    "            'low': np.array(prices) * (1 - np.abs(np.random.normal(0, 0.002, len(prices)))),\n",
    "            'close': prices,\n",
    "            'volume': np.random.gamma(2, 100000, len(prices)).astype(int)\n",
    "        })\n",
    "        \n",
    "        # Ensure OHLC consistency\n",
    "        df['high'] = df[['open', 'high', 'close']].max(axis=1)\n",
    "        df['low'] = df[['open', 'low', 'close']].min(axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fetch_ibkr_data(self, ticker, ipo_date):\n",
    "        \"\"\"Fetch data from IBKR with retry logic\"\"\"\n",
    "        try:\n",
    "            from ib_insync import IB, Stock, util\n",
    "            \n",
    "            ib = IB()\n",
    "            ib.connect(\n",
    "                self.config['IBKR']['HOST'],\n",
    "                self.config['IBKR']['PORT'],\n",
    "                clientId=self.config['IBKR']['CLIENT_ID'] + random.randint(0, 100)\n",
    "            )\n",
    "            \n",
    "            contract = Stock(ticker, 'SMART', 'USD')\n",
    "            qualified = ib.qualifyContracts(contract)\n",
    "            \n",
    "            if qualified:\n",
    "                eastern = pytz.timezone('America/New_York')\n",
    "                end_dt = eastern.localize(ipo_date.replace(hour=23, minute=59))\n",
    "                end_str = end_dt.strftime(\"%Y%m%d %H:%M:%S\") + \" US/Eastern\"\n",
    "                \n",
    "                bars = ib.reqHistoricalData(\n",
    "                    qualified[0],\n",
    "                    endDateTime=end_str,\n",
    "                    durationStr='1 D',\n",
    "                    barSizeSetting='1 min',\n",
    "                    whatToShow='TRADES',\n",
    "                    useRTH=False,\n",
    "                    formatDate=1\n",
    "                )\n",
    "                \n",
    "                if bars:\n",
    "                    df = util.df(bars)\n",
    "                    df['datetime'] = pd.to_datetime(df['date'])\n",
    "                    df = df.drop('date', axis=1)\n",
    "                    ib.disconnect()\n",
    "                    return df\n",
    "            \n",
    "            ib.disconnect()\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   IBKR error for {ticker}: {str(e)[:50]}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_data(self, universe):\n",
    "        \"\"\"Main data collection with checkpoint support\"\"\"\n",
    "        print(f\"\\nüì° Starting data collection...\")\n",
    "        print(f\"   Mode: {self.config['DATA_MODE']}\")\n",
    "        print(f\"   Already processed: {len(self.checkpoint['processed'])}\")\n",
    "        \n",
    "        # Filter out already processed\n",
    "        remaining = universe[~universe['Ticker'].isin(self.checkpoint['processed'])]\n",
    "        print(f\"   Remaining: {len(remaining)}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = self.config['BATCH_SIZE']\n",
    "        collected_data = {}\n",
    "        \n",
    "        for batch_idx in range(0, len(remaining), batch_size):\n",
    "            batch = remaining.iloc[batch_idx:batch_idx + batch_size]\n",
    "            print(f\"\\nüì¶ Processing batch {batch_idx//batch_size + 1}/{(len(remaining)-1)//batch_size + 1}\")\n",
    "            \n",
    "            for _, row in tqdm(batch.iterrows(), total=len(batch), desc=\"Collecting\"):\n",
    "                ticker = row['Ticker']\n",
    "                \n",
    "                # Check if already saved\n",
    "                ticker_file = self.data_dir / f\"{ticker}.csv\"\n",
    "                if ticker_file.exists():\n",
    "                    df = pd.read_csv(ticker_file)\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                    collected_data[ticker] = df\n",
    "                    continue\n",
    "                \n",
    "                # Collect data based on mode\n",
    "                df = None\n",
    "                \n",
    "                if self.config['DATA_MODE'] in ['IBKR', 'HYBRID']:\n",
    "                    # Try IBKR first\n",
    "                    for attempt in range(self.config['IBKR']['MAX_RETRIES']):\n",
    "                        df = self.fetch_ibkr_data(ticker, row['IPO_Date'])\n",
    "                        if df is not None:\n",
    "                            break\n",
    "                        \n",
    "                        # Exponential backoff\n",
    "                        delay = self.config['IBKR']['REQUEST_DELAY'] * \\\n",
    "                                (self.config['IBKR']['BACKOFF_FACTOR'] ** attempt)\n",
    "                        time.sleep(delay)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        # Success with IBKR\n",
    "                        time.sleep(self.config['IBKR']['REQUEST_DELAY'])\n",
    "                    elif self.config['DATA_MODE'] == 'HYBRID':\n",
    "                        # Fallback to simulation\n",
    "                        df = self.generate_ipo_day_data(\n",
    "                            ticker, row['IPO_Date'], row['IPO_Price']\n",
    "                        )\n",
    "                else:\n",
    "                    # Pure simulation mode\n",
    "                    df = self.generate_ipo_day_data(\n",
    "                        ticker, row['IPO_Date'], row['IPO_Price']\n",
    "                    )\n",
    "                \n",
    "                if df is not None:\n",
    "                    # Save data\n",
    "                    df.to_csv(ticker_file, index=False)\n",
    "                    collected_data[ticker] = df\n",
    "                    self.checkpoint['processed'].append(ticker)\n",
    "                else:\n",
    "                    self.checkpoint['failed'].append(ticker)\n",
    "                \n",
    "                # Update checkpoint\n",
    "                self.checkpoint['last_processed'] = ticker\n",
    "                self.checkpoint['total_requests'] += 1\n",
    "                \n",
    "                # Save checkpoint every 10 tickers\n",
    "                if len(self.checkpoint['processed']) % 10 == 0:\n",
    "                    self.save_checkpoint()\n",
    "            \n",
    "            # Pacing delay between batches\n",
    "            if batch_idx + batch_size < len(remaining):\n",
    "                print(f\"   Waiting {self.config['IBKR']['PACING_DELAY']}s before next batch...\")\n",
    "                time.sleep(self.config['IBKR']['PACING_DELAY'])\n",
    "        \n",
    "        # Final checkpoint save\n",
    "        self.save_checkpoint()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Data collection complete!\")\n",
    "        print(f\"   Successful: {len(collected_data)}\")\n",
    "        print(f\"   Failed: {len(self.checkpoint['failed'])}\")\n",
    "        \n",
    "        return collected_data\n",
    "\n",
    "# Initialize collector and run\n",
    "collector = IPODataCollector(test_dir, CONFIG)\n",
    "ipo_data = collector.collect_data(ipo_universe)\n",
    "\n",
    "print(f\"\\nüìä Collected data for {len(ipo_data)} tickers\")\n",
    "print(f\"üíæ Data saved to {test_dir}/ticker_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Window Analysis Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trading_windows_enhanced(data_dict, metadata):\n",
    "    \"\"\"Enhanced window analysis with metadata tracking\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Analyzing {len(data_dict)} tickers across trading windows...\")\n",
    "    \n",
    "    # Generate all possible windows\n",
    "    times = []\n",
    "    for hour in range(9, 16):\n",
    "        for minute in [0, 30]:\n",
    "            if hour == 9 and minute == 0:\n",
    "                continue\n",
    "            if hour == 16 and minute == 30:\n",
    "                continue\n",
    "            times.append(f\"{hour:02d}:{minute:02d}\")\n",
    "    \n",
    "    results = []\n",
    "    window_details = {}  # Store per-ticker results\n",
    "    \n",
    "    total_windows = sum(1 for i in range(len(times)-1) for _ in times[i+1:])\n",
    "    \n",
    "    with tqdm(total=total_windows, desc=\"Analyzing windows\") as pbar:\n",
    "        for i, buy_time_str in enumerate(times[:-1]):\n",
    "            for sell_time_str in times[i+1:]:\n",
    "                buy_time = pd.to_datetime(buy_time_str).time()\n",
    "                sell_time = pd.to_datetime(sell_time_str).time()\n",
    "                window_key = f\"{buy_time_str}-{sell_time_str}\"\n",
    "                \n",
    "                returns = []\n",
    "                ticker_returns = {}\n",
    "                \n",
    "                for ticker, df in data_dict.items():\n",
    "                    # Find prices at specified times\n",
    "                    buy_mask = df['datetime'].dt.time == buy_time\n",
    "                    sell_mask = df['datetime'].dt.time == sell_time\n",
    "                    \n",
    "                    if buy_mask.any() and sell_mask.any():\n",
    "                        buy_price = df.loc[buy_mask, 'close'].iloc[0]\n",
    "                        sell_price = df.loc[sell_mask, 'close'].iloc[0]\n",
    "                        \n",
    "                        if buy_price > 0:\n",
    "                            ret = (sell_price - buy_price) / buy_price\n",
    "                            returns.append(ret)\n",
    "                            ticker_returns[ticker] = ret\n",
    "                \n",
    "                if len(returns) >= 5:  # Minimum sample size\n",
    "                    duration = (datetime.strptime(sell_time_str, '%H:%M') - \n",
    "                              datetime.strptime(buy_time_str, '%H:%M')).seconds / 3600\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    avg_return = np.mean(returns) * 100\n",
    "                    median_return = np.median(returns) * 100\n",
    "                    std_return = np.std(returns) * 100\n",
    "                    win_rate = sum(1 for r in returns if r > 0) / len(returns) * 100\n",
    "                    sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
    "                    \n",
    "                    # Calculate additional metrics\n",
    "                    skew = pd.Series(returns).skew()\n",
    "                    kurtosis = pd.Series(returns).kurtosis()\n",
    "                    max_drawdown = min(returns) * 100\n",
    "                    best_ticker = max(ticker_returns.items(), key=lambda x: x[1])[0]\n",
    "                    worst_ticker = min(ticker_returns.items(), key=lambda x: x[1])[0]\n",
    "                    \n",
    "                    results.append({\n",
    "                        'window': window_key,\n",
    "                        'buy_time': buy_time_str,\n",
    "                        'sell_time': sell_time_str,\n",
    "                        'duration_hrs': duration,\n",
    "                        'n_tickers': len(returns),\n",
    "                        'avg_return': avg_return,\n",
    "                        'median_return': median_return,\n",
    "                        'std_return': std_return,\n",
    "                        'win_rate': win_rate,\n",
    "                        'sharpe': sharpe,\n",
    "                        'max_return': max(returns) * 100,\n",
    "                        'min_return': min(returns) * 100,\n",
    "                        'skew': skew,\n",
    "                        'kurtosis': kurtosis,\n",
    "                        'return_per_hour': avg_return / duration if duration > 0 else 0,\n",
    "                        'best_ticker': best_ticker,\n",
    "                        'worst_ticker': worst_ticker\n",
    "                    })\n",
    "                    \n",
    "                    # Store detailed results\n",
    "                    window_details[window_key] = ticker_returns\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    results_df = pd.DataFrame(results).sort_values('avg_return', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Add metadata\n",
    "    metadata['windows_analyzed'] = len(results_df)\n",
    "    metadata['best_window'] = results_df.iloc[0]['window'] if len(results_df) > 0 else None\n",
    "    metadata['best_return'] = results_df.iloc[0]['avg_return'] if len(results_df) > 0 else None\n",
    "    metadata['best_sharpe'] = results_df['sharpe'].max() if len(results_df) > 0 else None\n",
    "    metadata['highest_win_rate'] = results_df['win_rate'].max() if len(results_df) > 0 else None\n",
    "    \n",
    "    return results_df, window_details, metadata\n",
    "\n",
    "# Run enhanced analysis\n",
    "metadata = {\n",
    "    'test_timestamp': timestamp,\n",
    "    'tickers_analyzed': len(ipo_data),\n",
    "    'date_range': f\"{CONFIG['START_DATE']} to {CONFIG['END_DATE']}\",\n",
    "    'data_mode': CONFIG['DATA_MODE']\n",
    "}\n",
    "\n",
    "results_df, window_details, metadata = analyze_trading_windows_enhanced(ipo_data, metadata)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(test_dir / 'window_analysis_results.csv', index=False)\n",
    "with open(test_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete!\")\n",
    "print(f\"   Windows analyzed: {len(results_df)}\")\n",
    "print(f\"   Best window: {metadata['best_window']}\")\n",
    "print(f\"   Best return: {metadata['best_return']:.2f}%\")\n",
    "print(f\"\\nüíæ Results saved to {test_dir}\")\n",
    "\n",
    "# Display top results\n",
    "print(\"\\nüèÜ TOP 10 WINDOWS:\")\n",
    "results_df.head(10)[['window', 'avg_return', 'win_rate', 'sharpe', 'n_tickers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Generate Abstract & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract(results_df, metadata, test_dir):\n",
    "    \"\"\"Generate high-level abstract of findings\"\"\"\n",
    "    \n",
    "    # Classify results\n",
    "    profitable_windows = len(results_df[results_df['avg_return'] > 0])\n",
    "    high_win_rate = len(results_df[results_df['win_rate'] > 60])\n",
    "    \n",
    "    # Determine overall assessment\n",
    "    if profitable_windows > len(results_df) * 0.7:\n",
    "        assessment = \"STRONG POSITIVE\"\n",
    "        emoji = \"üü¢\"\n",
    "    elif profitable_windows > len(results_df) * 0.4:\n",
    "        assessment = \"MIXED\"\n",
    "        emoji = \"üü°\"\n",
    "    else:\n",
    "        assessment = \"WEAK\"\n",
    "        emoji = \"üî¥\"\n",
    "    \n",
    "    # Key insights\n",
    "    top3 = results_df.head(3)\n",
    "    morning_windows = results_df[results_df['buy_time'].apply(lambda x: int(x.split(':')[0]) < 12)]\n",
    "    afternoon_windows = results_df[results_df['buy_time'].apply(lambda x: int(x.split(':')[0]) >= 12)]\n",
    "    \n",
    "    abstract = f\"\"\"# IPO Farming Backtest Abstract\n",
    "\n",
    "## Test Information\n",
    "- **Test ID**: {metadata['test_timestamp']}\n",
    "- **Date Range**: {metadata['date_range']}\n",
    "- **Tickers Analyzed**: {metadata['tickers_analyzed']}\n",
    "- **Windows Tested**: {metadata['windows_analyzed']}\n",
    "- **Data Source**: {metadata['data_mode']}\n",
    "\n",
    "## Overall Assessment: {emoji} {assessment}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Performance Summary\n",
    "- **Profitable Windows**: {profitable_windows}/{len(results_df)} ({profitable_windows/len(results_df)*100:.1f}%)\n",
    "- **High Win Rate (>60%)**: {high_win_rate} windows\n",
    "- **Best Return**: {results_df.iloc[0]['avg_return']:.2f}% ({results_df.iloc[0]['window']})\n",
    "- **Best Sharpe**: {results_df['sharpe'].max():.2f}\n",
    "- **Highest Win Rate**: {results_df['win_rate'].max():.1f}%\n",
    "\n",
    "### Top 3 Strategies\n",
    "1. **{top3.iloc[0]['window']}**: {top3.iloc[0]['avg_return']:.2f}% return, {top3.iloc[0]['win_rate']:.1f}% win rate\n",
    "2. **{top3.iloc[1]['window']}**: {top3.iloc[1]['avg_return']:.2f}% return, {top3.iloc[1]['win_rate']:.1f}% win rate\n",
    "3. **{top3.iloc[2]['window']}**: {top3.iloc[2]['avg_return']:.2f}% return, {top3.iloc[2]['win_rate']:.1f}% win rate\n",
    "\n",
    "### Time-Based Insights\n",
    "- **Morning Entry (before 12:00)**: Avg return = {morning_windows['avg_return'].mean():.2f}%\n",
    "- **Afternoon Entry (after 12:00)**: Avg return = {afternoon_windows['avg_return'].mean():.2f}%\n",
    "- **Optimal Entry Hour**: {results_df.groupby(results_df['buy_time'].apply(lambda x: x.split(':')[0]))['avg_return'].mean().idxmax()}:00-:30\n",
    "- **Optimal Exit Hour**: {results_df.groupby(results_df['sell_time'].apply(lambda x: x.split(':')[0]))['avg_return'].mean().idxmax()}:00-:30\n",
    "\n",
    "### Risk Analysis\n",
    "- **Average Volatility**: {results_df['std_return'].mean():.2f}%\n",
    "- **Worst Drawdown**: {results_df['min_return'].min():.2f}%\n",
    "- **Average Skew**: {results_df['skew'].mean():.2f}\n",
    "- **Risk-Adjusted Best**: {results_df.nlargest(1, 'sharpe').iloc[0]['window']} (Sharpe: {results_df['sharpe'].max():.2f})\n",
    "\n",
    "## Interesting Observations\n",
    "\"\"\"\n",
    "    \n",
    "    # Add interesting observations\n",
    "    observations = []\n",
    "    \n",
    "    # Check for patterns\n",
    "    if morning_windows['avg_return'].mean() > afternoon_windows['avg_return'].mean() * 1.5:\n",
    "        observations.append(\"- Morning entries significantly outperform afternoon entries\")\n",
    "    \n",
    "    short_holds = results_df[results_df['duration_hrs'] <= 2]\n",
    "    long_holds = results_df[results_df['duration_hrs'] > 4]\n",
    "    if short_holds['avg_return'].mean() > long_holds['avg_return'].mean():\n",
    "        observations.append(\"- Shorter holding periods show better returns than longer holds\")\n",
    "    \n",
    "    if results_df.iloc[0]['win_rate'] > 70:\n",
    "        observations.append(f\"- Top strategy shows exceptional consistency with {results_df.iloc[0]['win_rate']:.1f}% win rate\")\n",
    "    \n",
    "    if len(observations) > 0:\n",
    "        abstract += \"\\n\".join(observations)\n",
    "    else:\n",
    "        abstract += \"- No exceptional patterns detected in this test\"\n",
    "    \n",
    "    abstract += f\"\"\"\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Based on this analysis:\n",
    "1. **Entry Time**: Consider entering positions around {results_df.iloc[0]['buy_time']}\n",
    "2. **Exit Time**: Target exits around {results_df.iloc[0]['sell_time']}\n",
    "3. **Hold Duration**: Optimal holding period appears to be {results_df.iloc[0]['duration_hrs']:.1f} hours\n",
    "4. **Risk Management**: Use stop losses due to {abs(results_df['min_return'].min()):.1f}% worst-case drawdown\n",
    "5. **Position Sizing**: Consider win rate of {results_df.iloc[0]['win_rate']:.1f}% when sizing positions\n",
    "\n",
    "## Test Metadata\n",
    "- **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Test Directory**: {test_dir.name}\n",
    "- **Status**: Complete\n",
    "\"\"\"\n",
    "    \n",
    "    # Save abstract\n",
    "    with open(test_dir / 'abstract.md', 'w') as f:\n",
    "        f.write(abstract)\n",
    "    \n",
    "    return abstract\n",
    "\n",
    "# Generate and display abstract\n",
    "abstract = generate_abstract(results_df, metadata, test_dir)\n",
    "print(abstract)\n",
    "print(f\"\\nüíæ Abstract saved to {test_dir}/abstract.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Master Results Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master_log(test_dir, metadata, results_df):\n",
    "    \"\"\"Update master log with test results\"\"\"\n",
    "    \n",
    "    master_log_file = Path(CONFIG['OUTPUT_BASE']) / 'master_results_log.csv'\n",
    "    \n",
    "    # Create entry for this test\n",
    "    entry = {\n",
    "        'test_id': metadata['test_timestamp'],\n",
    "        'test_dir': test_dir.name,\n",
    "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'time': datetime.now().strftime('%H:%M:%S'),\n",
    "        'tickers_analyzed': metadata['tickers_analyzed'],\n",
    "        'date_range': metadata['date_range'],\n",
    "        'data_mode': metadata['data_mode'],\n",
    "        'windows_analyzed': metadata['windows_analyzed'],\n",
    "        'best_window': metadata.get('best_window'),\n",
    "        'best_return': metadata.get('best_return'),\n",
    "        'best_sharpe': metadata.get('best_sharpe'),\n",
    "        'highest_win_rate': metadata.get('highest_win_rate'),\n",
    "        'profitable_windows': len(results_df[results_df['avg_return'] > 0]),\n",
    "        'avg_return_all': results_df['avg_return'].mean(),\n",
    "        'status': 'Complete'\n",
    "    }\n",
    "    \n",
    "    # Load existing log or create new\n",
    "    if master_log_file.exists():\n",
    "        master_log = pd.read_csv(master_log_file)\n",
    "        master_log = pd.concat([master_log, pd.DataFrame([entry])], ignore_index=True)\n",
    "    else:\n",
    "        master_log = pd.DataFrame([entry])\n",
    "    \n",
    "    # Save updated log\n",
    "    master_log.to_csv(master_log_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüìã Master Log Updated\")\n",
    "    print(f\"   Total tests: {len(master_log)}\")\n",
    "    print(f\"   Location: {master_log_file}\")\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"\\nüìä Comparison with Previous Tests:\")\n",
    "    print(master_log[['test_id', 'tickers_analyzed', 'best_window', 'best_return', 'best_sharpe']].tail(5))\n",
    "    \n",
    "    return master_log\n",
    "\n",
    "# Update master log\n",
    "if CONFIG['TRACK_MASTER_LOG']:\n",
    "    master_log = update_master_log(test_dir, metadata, results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \"*25 + \"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä TEST SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Test ID: {metadata['test_timestamp']}\")\n",
    "print(f\"   ‚Ä¢ IPOs Analyzed: {metadata['tickers_analyzed']}\")\n",
    "print(f\"   ‚Ä¢ Windows Tested: {metadata['windows_analyzed']}\")\n",
    "print(f\"   ‚Ä¢ Best Strategy: {metadata['best_window']}\")\n",
    "print(f\"   ‚Ä¢ Expected Return: {metadata['best_return']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT LOCATION:\")\n",
    "print(f\"   {test_dir}\")\n",
    "\n",
    "print(f\"\\nüìÑ FILES GENERATED:\")\n",
    "files = [\n",
    "    'config.json',\n",
    "    'ipo_universe.csv', \n",
    "    'window_analysis_results.csv',\n",
    "    'metadata.json',\n",
    "    'abstract.md',\n",
    "    'checkpoint.json'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    file_path = test_dir / file\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size / 1024\n",
    "        print(f\"   ‚úì {file:30s} ({size:6.1f} KB)\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Review abstract.md for high-level findings\")\n",
    "print(\"   2. Analyze window_analysis_results.csv for detailed metrics\")\n",
    "print(\"   3. Compare with previous tests in master_results_log.csv\")\n",
    "print(\"   4. Run visualization notebook for charts and graphs\")\n",
    "print(\"   5. Test top strategies with paper trading\")\n",
    "\n",
    "print(f\"\\nüí° TO RUN ANOTHER TEST:\")\n",
    "print(\"   1. Modify CONFIG settings in cell 2\")\n",
    "print(\"   2. Re-run all cells\")\n",
    "print(\"   3. Results will be saved in new timestamped directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Thank you for using IPO Farming Backtest System!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}